{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo NLP: Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'module_4/part_05'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subimos los datos a s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-1-467432373215/module_4/part_05/train/dbpedia.train'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_channel = prefix + '/train'\n",
    "sess.upload_data(path='dbpedia_csv/dbpedia.train', bucket=bucket, key_prefix=train_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-1-467432373215/module_4/part_05/validation/dbpedia.validation'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_channel = prefix + '/validation'\n",
    "sess.upload_data(path='dbpedia_csv/dbpedia.validation', bucket=bucket, key_prefix=validation_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo\n",
    "\n",
    "- Usaremos el modelo https://docs.aws.amazon.com/es_es/sagemaker/latest/dg/blazingtext.html\n",
    "\n",
    "- Amazon SageMaker BlazingText proporciona implementaciones altamente optimizadas de los algoritmos de Word2VEC y de clasificación de texto. El algoritmo Word2vec es útil para muchas tareas de procesamiento de lenguaje natural (NLP) posteriores, como por ejemplo, análisis del sentimiento de reconocimiento de entidad nombrada, traducción automática, etc. La clasificación de texto es una tarea importante para las aplicaciones que realizan búsquedas web, recuperación de información, funciones de clasificación y clasificación de documentos.\n",
    "\n",
    "- El algoritmo Word2vec asigna palabras a vectores distribuidos de alta calidad. La representación vectorial resultante de una palabra se denomina una incrustación de palabra. Las palabras similares desde el punto de vista semántico corresponden a vectores que se acercan entre sí. De esta forma, las incrustaciones de palabras capturan relaciones semánticas entre palabras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685385470294.dkr.ecr.eu-west-1.amazonaws.com/blazingtext:1\n"
     ]
    }
   ],
   "source": [
    "image=sagemaker.image_uris.retrieve(framework='blazingtext', \n",
    "                                    region=region, \n",
    "                                    version='1')\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "        image,\n",
    "        role,\n",
    "        instance_count=1,\n",
    "        instance_type='ml.c5.2xlarge',\n",
    "        volume_size=30,\n",
    "        max_run=360000,\n",
    "        input_mode='File',\n",
    "        enable_sagemaker_metrics=True,\n",
    "        output_path=s3_output_location,\n",
    "        hyperparameters={\n",
    "            'mode': 'supervised',\n",
    "            'epochs': 20,\n",
    "            'min_count': 2,\n",
    "            'learning_rate': 0.05,\n",
    "            'vector_dim': 10,\n",
    "            'early_stopping': True,\n",
    "            'patience': 4,\n",
    "            'min_epochs': 5,\n",
    "            'word_ngrams': 2,\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-west-1-467432373215/module_4/part_05/train\n",
      "s3://sagemaker-eu-west-1-467432373215/module_4/part_05/validation\n"
     ]
    }
   ],
   "source": [
    "s3_train_data = f's3://{bucket}/{train_channel}'\n",
    "s3_validation_data = f's3://{bucket}/{validation_channel}'\n",
    "print(s3_train_data)\n",
    "print(s3_validation_data)\n",
    "\n",
    "data_channels = {'train': s3_train_data, \n",
    "                 'validation': s3_validation_data}\n",
    "\n",
    "jobname = f'dbpedia-blazingtext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-13 21:48:35 Starting - Starting the training job...\n",
      "2022-12-13 21:48:58 Starting - Preparing the instances for trainingProfilerReport-1670968115: InProgress\n",
      "......\n",
      "2022-12-13 21:50:03 Downloading - Downloading input data...\n",
      "2022-12-13 21:50:19 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[12/13/2022 21:50:20 WARNING 140110651049792] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[12/13/2022 21:50:20 WARNING 140110651049792] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[12/13/2022 21:50:20 INFO 140110651049792] nvidia-smi took: 0.025187015533447266 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[12/13/2022 21:50:20 INFO 140110651049792] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34mNumber of CPU sockets found in instance is  1\u001b[0m\n",
      "\u001b[34m[12/13/2022 21:50:20 INFO 140110651049792] Processing /opt/ml/input/data/train/dbpedia.train . File size: 34.936402320861816 MB\u001b[0m\n",
      "\u001b[34m[12/13/2022 21:50:20 INFO 140110651049792] Processing /opt/ml/input/data/validation/dbpedia.validation . File size: 21.887572288513184 MB\u001b[0m\n",
      "\u001b[34mRead 6M words\u001b[0m\n",
      "\u001b[34mNumber of words:  148514\u001b[0m\n",
      "\u001b[34mLoading validation data from /opt/ml/input/data/validation/dbpedia.validation\u001b[0m\n",
      "\u001b[34mLoaded validation data.\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 2\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0417  Progress: 16.55%  Million Words/sec: 21.34 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 3\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 4\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0389  Progress: 22.11%  Million Words/sec: 21.74 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 5\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.970829\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0362  Progress: 27.59%  Million Words/sec: 19.21 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 6\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.972429\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0334  Progress: 33.29%  Million Words/sec: 17.87 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 7\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.973071\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0305  Progress: 39.02%  Million Words/sec: 17.03 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 8\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.974229\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0276  Progress: 44.88%  Million Words/sec: 16.47 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 9\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.974429\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0247  Progress: 50.67%  Million Words/sec: 16.07 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 10\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.974771\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0218  Progress: 56.31%  Million Words/sec: 15.66 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 11\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.9752\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 12\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.975957\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0189  Progress: 62.23%  Million Words/sec: 14.79 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 13\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.975886\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0162  Progress: 67.68%  Million Words/sec: 14.79 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 14\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.976314\u001b[0m\n",
      "\u001b[34mValidation accuracy improved! Storing best weights...\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0132  Progress: 73.60%  Million Words/sec: 14.68 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 15\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.976286\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 1 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0106  Progress: 78.86%  Million Words/sec: 14.66 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 16\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.9763\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 2 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0077  Progress: 84.52%  Million Words/sec: 14.72 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 17\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.976171\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 3 epochs.\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0051  Progress: 89.83%  Million Words/sec: 14.70 #####\u001b[0m\n",
      "\u001b[34m-------------- End of epoch: 18\u001b[0m\n",
      "\u001b[34mUsing 8 threads for prediction!\u001b[0m\n",
      "\u001b[34mValidation accuracy: 0.976257\u001b[0m\n",
      "\u001b[34mValidation accuracy has not improved for last 4 epochs.\u001b[0m\n",
      "\u001b[34mReached patience. Terminating training.\u001b[0m\n",
      "\u001b[34mBest epoch: 14\u001b[0m\n",
      "\u001b[34mBest validation accuracy: 0.976314\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 15.82 #####\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 15.82\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 7.85\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.9953\u001b[0m\n",
      "\u001b[34mNumber of train examples: 112000\u001b[0m\n",
      "\u001b[34m#validation_accuracy: 0.9763\u001b[0m\n",
      "\u001b[34mNumber of validation examples: 70000\u001b[0m\n",
      "\n",
      "2022-12-13 21:50:59 Uploading - Uploading generated training model\n",
      "2022-12-13 21:50:59 Completed - Training job completed\n",
      "Training seconds: 52\n",
      "Billable seconds: 52\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs=data_channels,\n",
    "              job_name=jobname,\n",
    "              logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos acceder al modelo con estimator.model_data\n",
    "- Este modelo lo podemos desplegar como veremos más adelante o descargarlo en nuestro ordenador y usar el programa o SDK de fastText (https://fasttext.cc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-west-1-467432373215/module_4/part_05/output/dbpedia-blazingtext/output/model.tar.gz'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {estimator.model_data} ./dbpedia_csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd dbpedia_csv/\n",
    "tar -zxf model.tar.gz"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
